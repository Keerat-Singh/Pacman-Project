Have made multiple changes on DQN alog:

1. Started from a simple 3 layer NN with no ReplayMemory 
2. Added ReplayMemory to keep track of random sample of records
3. Added a dropout layer to avoid overfitting and make model more robust
4. Updated episode to 15000 to see how far the agent can understand. But, it agent was stuck and not learning properly
5. Update epsilon implementation, batch size and Replay Memory buffer size
6. Agent was still not learning enough, so removed dropout layer as it might be causing underfitting 